#Installation de pyspark
#! pip install pyspark
#importation des packages
import pyspark
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Crée un spack
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('TP').getOrCreate()
# Affichage des informations de la session
spark
#Importation des datasets
data1 = spark.read.option('header', 'true').option('delimiter', ';').option('inferSchema', 'true').csv('/content/TP/articles.csv')
data2 = spark.read.option('header', 'true').option('delimiter', ';').option('inferSchema', 'true').csv('/content/TP/ventes.csv')
# Format des bases
print("Format de la base des articles:",(data1.count(), len(data1.columns)))
print("Format de la base des ventes:",(data2.count(), len(data2.columns)))
Format de la base des articles: (914976, 3)
Format de la base des ventes: (376874, 8)
# Vérification du type de données : ARTICLES
data1.printSchema()
# Vérification du type de données: VENTES
data2.printSchema()
# Aperçu des données
data1.show(2)
data2.show(2)
#Statistiques descriptives pour la table des articles
data1.describe().show()
#Statistiques descriptives pour la table des ventes
data2.describe().show()
# Unifier le type de la colonne ARTICLE
data2=data2.withColumn("ARTICLE", data2['ARTICLE'].cast("string"))
#Verification
data2.printSchema()
 3. Jointure des deux tables
data=data2.join(data1, on='ARTICLE', how='left')
data.show(5)
#Verification du format de la base jointe
print("Format de la base jointe:",(data.count(), len(data.columns)))
Format de la base jointe: (376874, 10)
 4. Visualisation des données
#selection de la colonne à visualiser
bplot = data.select("QTE").toPandas()
#type de visualisation : boxplot
bplot.plot(kind='box')
 5. Suppression des valeurs abérrantes et des quantités égales à 0
data_mod=data.filter((data['QTE']< 200))
data_mod=data_mod.filter((data_mod['QTE']>0))
data_mod.show(5)
#Verification du format de la base
print("Format de la nouvelle base :", (data_mod.count(), len(data_mod.columns)))
 Vérification des valeurs nulles
from pyspark.sql.functions import isnan, when, count, col
from pyspark.sql.functions import col,sum
data_mod.select(*(sum(col(c).isNull().cast("int")).alias(c) for c in data_mod.columns)).show()
#Transformation de la colonne 'JOUR' en date
from pyspark.sql.functions import to_date, lit, year, month, weekofyear, dayofweek
data_mod = data_mod.withColumn('JOUR', to_date('JOUR', 'dd/MM/yy'))
#Verification des valeurs nulles
data_mod.select(*(sum(col(c).isNull().cast("int")).alias(c) for c in data_mod.columns)).show()
B. Modelisation
 1. Initialisation des variables
# Variable à expliquer
y=data_mod.select("QTE")
#Variables explicatives
X=data_mod.drop("QTE")
#Aperçu
y.show(5)
X.show(5)
 2. Séparation de la base en échantillon d'apprentissage(train) et de validation (test)
Commme nous travaillons sur les données temporelles, la base train est formé des données avant la date du 30/08/2019 et la
base test est faite des données d'après le 30/08/2019.
#Base train
x_train = X.filter(col("JOUR") <= lit('2019-08-30'))
y_train = y.filter(col("JOUR") <= lit('2019-08-30'))
#Base test
x_test = X.filter(col("JOUR") > lit('2019-08-30'))
y_test = y.filter(col("JOUR") > lit('2019-08-30'))
3. Extraction du jour, le mois et l'année de la variable "JOUR"
#Extraction de train
x_train = x_train.withColumn("year", year("JOUR")) \
                 .withColumn("month", month("JOUR")) \
                 .withColumn("week", weekofyear("JOUR")) \
                 .withColumn("day", dayofweek("JOUR"))
#extraction de test
x_test = x_test.withColumn("year", year("JOUR")) \
               .withColumn("month", month("JOUR")) \
               .withColumn("week", weekofyear("JOUR")) \
               .withColumn("day", dayofweek("JOUR"))
#day=jour de la semaine, week=numero de la semaine, month=le mois, year=l'année
#Aperçu
x_train.show(5)
x_test.show(5)

 4. Extraction des variables numériques
from pyspark.sql.types import NumericType
numerical_col = [f.name for f in x_train.schema.fields if isinstance(f.dataType, NumericType)]
numerical_col
['year', 'month', 'week', 'day']
 5. Transfromation en variable vectorielle
from pyspark.ml.feature import VectorAssembler, StandardScaler
assembler = VectorAssembler(inputCols=numerical_col, outputCol="numerical_features")
x_train = assembler.transform(x_train)
x_test = assembler.transform(x_test)

# Initialiser StandardScaler
scaler = StandardScaler(inputCol="numerical_features", outputCol="scaled_features")
# Ajuster StandardScaler sur les données d'entraînement
scalerModel = scaler.fit(x_train)
# Transformer les données d'entraînement et de test
x_train = scalerModel.transform(x_train)
x_test = scalerModel.transform(x_test)
#Vérificatio/Aperçu
x_train.show(5)
x_test.show(5)

#Extraction des variables catégorielles
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.sql.types import StringType
# Sélection des colonnes catégorielles
categorial_col = [f.name for f in x_train.schema.fields 
                  if isinstance(f.dataType, StringType) and f.name != 'ARTICLE']
categorial_col
['CATEG', 'SAISON', 'PERIODE', 'TRANCHE_PRIX', 'COULEUR', 'MATIERES', 'TYPE']
#Transformation des chaines de caractère en colonne d'indice
indexers = [StringIndexer(inputCol=c, outputCol="{0}_indexed".format(c)) for c in categorial_col]
#Transformation en variables vectorielles
assembler = VectorAssembler(inputCols=[indexers.getOutputCol() for indexers in indexers], outputCol="feature
#rassemblement
stages = indexers +  [assembler]
from pyspark.ml import Pipeline
# Création du pipeline
pipeline = Pipeline(stages=stages)
# Ajustement du pipeline sur les données d'entraînement
pipelineModel = pipeline.fit(x_train)
# Transformation des données d'entraînement
x_train_encoded = pipelineModel.transform(x_train)
#Aperçu de la nouvelle base
x_train_encoded.show(5)

 Application à la base test
# Ajustement du pipeline sur les données test
pipelineModel = pipeline.fit(x_test)
# Transformation des données test
x_test_encoded = pipelineModel.transform(x_test)
x_test_encoded.show(5)

6. Choix du modele
 Entrainement du modele
from pyspark.ml.regression import RandomForestRegressor
# Choix du modele : Random Forest
rf = RandomForestRegressor(featuresCol="features", labelCol="QTE", numTrees=10, maxDepth=10, minInstancesPerN
# Entraînement
df_train=x_train_encoded.join(y_train)
model = rf.fit(df_train)
 Prediction sur l'ensemble test
#Prédiction sur l'ensemble de test
df_test=x_test_encoded.join(y_test)
predictions = model.transform(x_test_encoded)
 Evaluation du modèle
# Évaluation du modèle
from pyspark.ml.evaluation import RegressionEvaluator
evaluator = RegressionEvaluator(labelCol="QTE", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print(f"Root Mean Squared Error (RMSE) on test data = {rmse}")

